{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "![Task1a](Task_1a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "![Task1b](Task_1b.png)\n",
    "\n",
    "## task 1c)\n",
    "\n",
    "![Task1c](Task_1c.png)\n",
    "\n",
    "## task 1d)\n",
    "\n",
    "![Task1d](Task_1d.png)\n",
    "## task 1e)\n",
    "\n",
    "![Task1e](Task_1e.png)\n",
    "## task 1f)\n",
    "\n",
    "![Task1f](Task_1f.png)\n",
    "## task 1g)\n",
    "\n",
    "![Task1g1](Task_1g1.png)\n",
    "![Task1g2](Task_1g2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "### Task 2a)\n",
    "![Task2a](plots\\task2_plot.png)\n",
    "\n",
    "### Task 2b)\n",
    "The final training accuracy was 0.8353.\\\n",
    "The final validation accuracy was 0.7344.\\\n",
    "The final test accuracy was 0.7251."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "For the first model I started by adding batch normalization. This increased the test accuracy, but not enough to get to 75% consistently. After that i tested different layer sizes and found that doubling the number of filters in the convolutional layers increased the accuracy further. Now the test accuracy was usually around 75% but not always. Finally i reduced the batch size to 32, which increased the accuracy further and gave a consistent test accuracy of 75%-77%. \n",
    "\n",
    "The following table shows my finished first model, where i used SGD as optimizer, learning rate of 0.05, batch size of 32 and default weight initialization. The convolutional layers used a 5x5 kernel, a stride of 1 and 2 layers of padding. The maxpool layers used a 2x2 kernel with a stride of 2 and no padding.\n",
    "\n",
    "| Layer | Layer Type | Number of Hidden Units / Number of Filters | Activation Function |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| 1 | Conv2d | 64 | ReLU |\n",
    "| 1 | BatchNorm2d | 64 | - |\n",
    "| 1 | MaxPool2d | - | - |\n",
    "| 2 | Conv2d | 128 | ReLU |\n",
    "| 2 | BatchNorm2d | 128 | - |\n",
    "| 2 | MaxPool2d | - | - |\n",
    "| 3 | Conv2d | 256 | ReLU |\n",
    "| 3 | BatchNorm2d | 256 | ReLU |\n",
    "| 3 | MaxPool2d | - | - |\n",
    "| --------- | --------- | --------- | --------- |\n",
    "|  | Flatten |  |  |\n",
    "| 4 | Fully-Connected | 64 | ReLU |\n",
    "| 4 | BatchNorm1d | 64 | - |\n",
    "| 5 | Fully-Connected | 10 | Softmax |\n",
    "\n",
    "For the second model I started by switching to the Adam optimizer. To get the new optimizer to converge I also had to reduce the learning rate to 0.001. Next I tested some different network architectures and ended up using a structure with two convolutional layers for each pooling layer. The model was not reaching satisfactory accuracy and it stopped very early, usually after 4 epochs. To avoid finding local minimas which makes it stop early I added dropout after every layer, but figured out the model performed better when I only had dropout after each convolutional layer and not in the fully connected layer. The dropout worked better when it had a low probability parameter, where the best value was 10%. The model was not performing well enough yet, with a test accuracy of 72%. The next change was reducing the kernel size of the convolutional layers to 3, which resulted in a test accuracy of 75% consistently. Finally I added batch normalization to see if it had as much impact as in model 1, and it increased the accuracy to 79%.\n",
    "\n",
    "The following table shows my finished second model, where i used Adam as optimizer, learning rate of 0.001, batch size of 64 and default weight initialization. The convolutional layers used a 3x3 kernel, a stride of 1 and 1 layer of padding. The maxpool layers used a 2x2 kernel with a stride of 2 and no padding. The dropout layers used a probability of 10%.\n",
    "\n",
    "| Layer | Layer Type | Number of Hidden Units / Number of Filters | Activation Function |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| 1 | Conv2d | 32 | ReLU |\n",
    "| 1 | Conv2d | 32 | ReLU |\n",
    "| 1 | BatchNorm2d | 32 | - |\n",
    "| 1 | MaxPool2d | - | - |\n",
    "| 1 | Dropout | - | - |\n",
    "| 2 | Conv2d | 64 | ReLU |\n",
    "| 2 | Conv2d | 64 | ReLU |\n",
    "| 1 | BatchNorm2d | 64 | - |\n",
    "| 2 | MaxPool2d | - | - |\n",
    "| 2 | Dropout | - | - |\n",
    "| 3 | Conv2d | 128 | ReLU |\n",
    "| 3 | Conv2d | 128 | ReLU |\n",
    "| 1 | BatchNorm2d | 128 | - |\n",
    "| 3 | MaxPool2d | - | - |\n",
    "| 3 | Dropout | - | - |\n",
    "| --------- | --------- | --------- | --------- |\n",
    "|  | Flatten |  |  |\n",
    "| 4 | Fully-Connected | 64 | ReLU |\n",
    "| 5 | Fully-Connected | 10 | Softmax |\n",
    "\n",
    "### Task 3b)\n",
    "\n",
    "| Model | Training loss | Training accuracy | Validation accuracy | Test accuracy |\n",
    "| :-: | :-: | :-: | :-: | :-: |\n",
    "| Model 1 | 0.4064 | 0.8647 | 0.7554 | 0.7522 |\n",
    "| Model 2 | 0.3459 | 0.8814 | 0.8038 | 0.7942 |\n",
    "\n",
    "![Task3b2](plots\\task3.2_plot.png)\n",
    "\n",
    "### Task 3c)\n",
    "I saw improvements with adding more nodes per layer, using batch normalization and using dropout. More nodes resulting in an improvement suggests that there were details in the data not being learned by the model from task 2, the model might have been underfitted, and the extra complexity from adding more nodes fixed this. The method which resulted in the greatest improvement was definitely batch normalization. It resulted in faster and more stable learning in a way no other method could. I am not entirely sure why, but it could be due to the normalization reducing the randomness introduced into the model from weight initialization. Dropout improved the model because it reduced overfitting drastically and reduced the risk of stopping too early due to local minima.\n",
    "\n",
    "Changes which did not improve the model was reducing the amount of nodes per layer and adding extra layers. As noted the model from task 2 showed signs of underfitting which means reducing complexity by reducing the amount of nodes would introduce even more underfitting. Starting out I thought adding extra layers would introduce the necessary model complexity, however the models became worse when i tried it. An explanation could be that the feature maps became too small to conserve the important information, as they would only have a size of 2x2 with an extra convolutional and maxpool layer. Adding another fully connected layer also reduced the model performance, which suggests that the underfitting was happening in the feature extractor and not in the classifier. Adding another fully connected layer in the classifier might have introduced overfitting in the classifier to reduce performance. \n",
    "\n",
    "### Task 3d)\n",
    "The method i saw the largest amount of improvement with was batch normalization.\n",
    "\n",
    "Model 2 before and after batch normalization:\n",
    "![Task3d](plots\\task3d_plot.png)\n",
    "\n",
    "### Task 3e)\n",
    "![Task3e](plots\\task3e_plot.png)\n",
    "\n",
    "The final test accuracy was 0.8117.\n",
    "\n",
    "### Task 3f)\n",
    "\n",
    "The final model shows signs of overfitting as we can clearly see the validation loss stagnating while the training loss keeps reducing. I tried reducing the overfitting by increasing the dropout probability, which made the validation loss follow the training loss longer, but that also reduced the test accuracy thus making a worse model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "FILL IN ANSWER. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "FILL IN ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "FILL IN ANSWER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
