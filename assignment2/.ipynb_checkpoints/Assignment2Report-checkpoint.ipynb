{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "![Task1a](Assignment2_Task1a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "![Task1b2](Assignment2_Task1b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![Task2c](task2c_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "There are $785*64 + 64x10 = 50880$ parameters in the network. The biases come from adding one extra input node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the changes were added incrementally and on top of eachother, e.g. the plot comparing sigmoid functions also has improved initial weights in both models.\n",
    "\n",
    "First i added improved initial weights which resulted in a much higher convergence speed, final accuracy and lower loss.\n",
    "\n",
    "![Task3a](task3_improved_weights.png)\n",
    "\n",
    "Next i added the improved sigmoid function which again gave a higher convergence speed, validation accuracy and lower training loss. The change was not as large as when adding the first trick but still a significant improvement.\n",
    "\n",
    "![Task3b](task3_improved_sigmoid.png)\n",
    "\n",
    "Finally i added momentum, which resulted in a lower training loss, but it reduced the validation accuracy by a tiny bit. This trick might have a better effect if the learning rate was set higher or if it was implemented without the improved weights and the improved sigmoid.\n",
    "\n",
    "![Task3c](task3_momentum.png)\n",
    "\n",
    "I tried increasing the learning rate to 0.03 while using momentum and the validation accuracy improved almost to the point of the network without momentum, but still not as good.\n",
    "![Task3c2](task3_momentum2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "For task 4 i chose to use the best performing model from task 3, which was the model with improved sigmoid but without momentum. This also reduced runtime and memory constraints which occured when trying to train multiple models, especially for the model with 128 hidden nodes which creates a lot of 128x20000 and 128x10000 arrays.\n",
    "\n",
    "With 32 hidden units we can see that the accuracy is lower than with 64, and the loss is also higher, which indicates some underfitting.\n",
    "\n",
    "![Task4a](task4_32vs64.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "With 128 hidden units we get a small increase in accuracy, however the training time and memory needed was a lot larger than with 64 units.\n",
    "\n",
    "![Task4b](task4_32vs64vs128.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "In task 3 the network had 50880 parameters (from 2d)). Now i want a network with two hidden layers of equal size which requires x nodes where x is determined by $x^2 + 785x + 10x = 50880.$ This equation translates to $x\\approx 59.5$ so i chose to make a network with two hidden layers of size 59, giving a total of $785*59 + 59^2 + 59*10 = 50386$ parameters.\n",
    "\n",
    "With two hidden layers we get a small increase in validation accuracy and a small increase in validation loss. The network had a higher validation accuracy with fewer parameters, but also a higher validation loss.\n",
    "\n",
    "![Task4d](task4d_multilayer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "I trained the model with ten hidden layers for 30 epochs. The plot compares the model with ten layers to the model with two layers and no momentum from Task 3.\n",
    "\n",
    "We can see that the model with ten hidden layers has a higher loss and lower accuracy than the two layer model. There is also a lot more noise in the curves. This is caused by overfitting to the noise in the Mnist pictures instead of using only the features important to distinguishing numbers. If we trained the ten layer model with more data it might have ended up with a different result, but for classifying numbers we do not need a model that complex in order to perform very well. For this application ten hidden layers will result in a model which is too complex and includes too much variance.\n",
    "\n",
    "![Task4e](task4e_multilayer.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
